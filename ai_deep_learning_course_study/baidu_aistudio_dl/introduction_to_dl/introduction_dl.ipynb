{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 确定模型参数\n",
    "\n",
    "这个有趣的案例演示了机器学习的基本过程，但其中有一个关键点的实现尚不清晰，即：如何确定模型参数$（w=1/m）$？\n",
    "\n",
    "确定参数的过程与科学家提出假说的方式类似，合理的假说至少可以解释所有的已知观测数据。如果未来观测到不符合理论假说的新数据，科学家会尝试提出新的假说。如天文史上，使用大圆和小圆组合的方式计算天体运行在中世纪是可以拟合观测数据的。但随着欧洲机械工业的进步，天文观测设备逐渐强大，越来越多的观测数据无法套用已有的理论，这促进了使用椭圆计算天体运行的理论假说出现。因此，**模型有效的基本条件是能够拟合已知的样本**，这给我们提供了学习有效模型的实现方案。\n",
    "\n",
    " **图3** 是以$H$为模型的假设，它是一个关于参数$W$和输入$X$的函数，用$H(W, X)$ 表示。模型的优化目标是$H(W, X)$的输出与真实输出$Y$尽量一致，两者的相差程度即是模型效果的评价函数（相差越小越好）。那么，确定参数的过程就是在已知的样本上，不断减小该评价函数（$H(W, X)$ 和$Y$的差距）的过程，直到学习到一个参数$W$，使得评价函数的取值最小。这个**衡量模型预测值和真实值差距的评价函数也被称为损失函数（损失Loss）**。\n",
    "\n",
    "<center><img src=\"./images/1.jpg\" width=\"300\" hegiht=\"\" ></center>\n",
    "<center><br>图3：确定模型参数示意图</br></center>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型结构介绍\n",
    "\n",
    "那么构成模型的三个部分（模型假设、评价函数和优化算法）是如何支撑机器学习流程的呢？如**图4** 所示：\n",
    "\n",
    "<center><img src=\"./images/2.png\" width=\"700\" hegiht=\"\" ></center>\n",
    "<center><br>图4：机器执行学习的框架</br></center>\n",
    "<br></br>\n",
    "\n",
    "* **模型假设**：世界上的可能关系千千万，漫无目标的试探$Y-X$之间的关系显然是十分低效的。因此假设空间先圈定了一个模型能够表达的关系可能，如蓝色圆圈所示。机器还会进一步在假设圈定的圆圈内寻找最优的$Y-X$关系，即确定参数$W$。\n",
    "* **评价函数**：寻找最优之前，我们需要先定义什么是最优，即评价一个$Y$~$X$关系的好坏的指标。通常衡量该关系是否能很好的拟合现有观测样本，将拟合的误差最小作为优化目标。\n",
    "* **优化算法**：设置了评价指标后，就可以在假设圈定的范围内，将使得评价指标最优（损失函数最小/最拟合已有观测样本）的$Y$~$X$关系找出来，这个寻找的方法即为优化算法。最笨的优化算法即按照参数的可能，穷举每一个可能取值来计算损失函数，保留使得损失函数最小的参数作为最终结果。\n",
    "\n",
    "从上述过程可以得出，机器学习的过程与牛顿第二定律的学习过程基本一致，都分为假设、评价和优化三个阶段：\n",
    "\n",
    "1. **假设**：通过观察加速度$a$和作用力$F$的观测数据，假设$a$和$F$是线性关系，即$a = w \\cdot F$。\n",
    "1. **评价**：对已知观测数据上的拟合效果好，即$w \\cdot F$计算的结果，要和观测的$a$尽量接近。\n",
    "1. **优化**：在参数$w$的所有可能取值中，发现$w=1/m$可使得评价最好（最拟合观测样本）。\n",
    "\n",
    "机器执行学习的框架体现了其**学习的本质是“参数估计”**（Learning is parameter estimation）。在此基础上，许多看起来完全不一样的问题都可以使用同样的框架进行学习，如科学定律、图像识别、机器翻译和自动问答等，它们的学习目标都是拟合一个“大公式”，如 **图5** 所示。\n",
    "\n",
    "\n",
    "<center><img src=\"./images/3.png\" width=\"500\" hegiht=\"\" ></center>\n",
    "<center><br>图5：机器学习就是拟合一个“大公式”</br></center>\n",
    "<br></br>\n",
    "\n",
    "他们的学习目标都是拟合一个“大公式”，公式中有很多可学习参数，机器学习的过程就是不断更新这些可学习参数。假定$f$是最终期望的\"公式\"函数，$g$是当前公式所拟合的函数。那么，机器学习的目的就是让假设$g$更加逼近期望目标$f$。\n",
    "<center><img src=\"./images/4.png\" width=\"600\" hegiht=\"\" ></center>\n",
    "<center><br>图6：使用数据去计算假设g去逼近目标f</br></center>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习\n",
    "\n",
    "机器学习算法理论在上个世纪90年代发展成熟，在许多领域都取得了成功应用。但平静的日子只延续到2010年左右，随着大数据的涌现和计算机算力提升，深度学习模型异军突起，极大改变了机器学习的应用格局。今天，多数机器学习任务都可以使用深度学习模型解决，尤其在语音、计算机视觉和自然语言处理等领域，深度学习模型的效果比传统机器学习算法有显著提升。\n",
    "\n",
    "那么相比传统的机器学习算法，深度学习做出了哪些改进呢？其实**两者在理论结构上是一致的，即：模型假设、评价函数和优化算法，其根本差别在于假设的复杂度**，如 **图6** 所示。\n",
    "\n",
    "<center><img src=\"./images/5.jpg\" width=\"1000\" hegiht=\"\" ></center>\n",
    "<center><br>图7：深度学习的模型复杂度难以想象</br></center>\n",
    "<br></br>\n",
    "\n",
    "不是所有的任务都像牛顿第二定律那样简单直观。对于 **图7** 中的美女照片，人脑可以接收到五颜六色的光学信号，能用极快的速度反应出这张图片是一位美女，而且是程序员喜欢的类型。但对计算机而言，只能接收到一个数字矩阵，对于美女这种高级的语义概念，从像素到高级语义概念中间要经历的信息变换的复杂性是难以想象的！这种变换已经无法用数学公式表达，因此研究者们借鉴了人脑神经元的结构，设计出神经网络的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络的基本概念\n",
    "\n",
    "人工神经网络包括多个神经网络层，如卷积层、全连接层、LSTM等，每一层又包括很多神经元，超过三层的非线性神经网络都可以被称为深度神经网络。通俗的讲，深度学习的模型可以视为是输入到输出的映射函数，如图像到高级语义（美女）的映射，足够深的神经网络理论上可以拟合任何复杂的函数。因此神经网络非常适合学习样本数据的内在规律和表示层次，对文字、图像和语音任务有很好的适用性。因为这几个领域的任务是人工智能的基础模块，所以深度学习被称为实现人工智能的基础也就不足为奇了。\n",
    "\n",
    "神经网络结构如 **图8** 所示。\n",
    "\n",
    "<center><img src=\"./images/6.png\" width=\"700\" hegiht=\"\" ></center>\n",
    "<center><br>图8：神经网络结构示意图</br></center>\n",
    "<br></br>\n",
    "\n",
    "* **神经元：** 神经网络中每个节点称为神经元，由两部分组成：\n",
    "  - 加权和：将所有输入加权求和。\n",
    "  - 非线性变换（激活函数）：加权和的结果经过一个非线性函数变换，让神经元计算具备非线性的能力。\n",
    "* **多层连接：** 大量这样的节点按照不同的层次排布，形成多层的结构连接起来，即称为神经网络。\n",
    "* **前向计算：** 从输入计算输出的过程，顺序从网络前至后。\n",
    "* **计算图：** 以图形化的方式展现神经网络的计算逻辑又称为计算图。我们也可以将神经网络的计算图以公式的方式表达，如下：\n",
    "$$Y =f_3 ( f_2 ( f_1 ( w_1\\cdot x_1+w_2\\cdot x_2+w_3\\cdot x_3+b ) + … ) … ) … )$$\n",
    "\n",
    "由此可见，神经网络并没有那么神秘，它的本质是一个含有很多参数的“大公式”。如果大家感觉这些概念仍过于抽象，理解的不够透彻，先不用着急，后续我们会以实践案例的方式，再次介绍这些概念。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习改变了AI应用的研发模式\n",
    "\n",
    "## 实现了端到端的学习\n",
    "\n",
    "深度学习改变了很多领域算法的实现模式。在深度学习兴起之前，很多领域建模的思路是投入大量精力做特征工程，将专家对某个领域的“人工理解”沉淀成特征表达，然后使用简单模型完成任务（如分类或回归）。而在数据充足的情况下，深度学习模型可以实现端到端的学习，即不需要专门做特征工程，将原始的特征输入模型中，模型可同时完成特征提取和分类任务，如 **图12** 所示。\n",
    "\n",
    "<center><img src=\"./images/7.png\" width=\"800\" hegiht=\"\" ></center>\n",
    "<center><br>图12：深度学习实现了端到端的学习</br></center>\n",
    "<br></br>\n",
    "\n",
    "以计算机视觉任务为例，特征工程是诸多图像科学家基于人类对视觉理论的理解，设计出来的一系列提取特征的计算步骤，典型如SIFT特征。在2010年之前的计算机视觉领域，人们普遍使用SIFT一类特征+SVM一类的简单浅层模型完成建模任务。\n",
    "\n",
    "------\n",
    "**说明：**\n",
    "\n",
    "SIFT特征由David Lowe在1999年提出，在2004年加以完善。SIFT特征是基于物体上的一些局部外观的兴趣点而与影像的大小和旋转无关。对于光线、噪声、微视角改变的容忍度也相当高。基于这些特性，它们是高度显著而且相对容易撷取，在母数庞大的特征数据库中，很容易辨识物体而且鲜有误认。使用SIFT特征描述对于部分物体遮蔽的侦测率也相当高，甚至只需要3个以上的SIFT物体特征就足以计算出位置与方位。在现今的电脑硬件速度下和小型的特征数据库条件下，辨识速度可接近即时运算。SIFT特征的信息量大，适合在海量数据库中快速准确匹配。\n",
    "\n",
    "------\n",
    "\n",
    "### 实现了深度学习框架标准化\n",
    "\n",
    "除了应用广泛的特点外，深度学习还推动人工智能进入工业大生产阶段，算法的通用性导致标准化、自动化和模块化的框架产生，如 **图13** 所示。\n",
    "\n",
    "<center><img src=\"./images/8.png\" width=\"600\" hegiht=\"\" ></center>\n",
    "<center><br>图13：深度学习模型具有通用性特点</br></center>\n",
    "<br></br>\n",
    "\n",
    "\n",
    "在此之前，不同流派的机器学习算法理论和实现均不同，导致每个算法均要独立实现，如随机森林和支撑向量机（SVM）。但在深度学习框架下，不同模型的算法结构有较大的通用性，如常用于计算机视觉的卷积神经网络模型（CNN）和常用于自然语言处理的长期短期记忆模型(LSTM)，都可以分为组网模块、梯度下降的优化模块和预测模块等。这使得抽象出统一的框架成为了可能，并大大降低了编写建模代码的成本。一些相对通用的模块，如网络基础算子的实现、各种优化算法等都可以由框架实现。建模者只需要关注数据处理，配置组网的方式，以及用少量代码串起训练和预测的流程即可。\n",
    "\n",
    "在深度学习框架出现之前，机器学习工程师处于手工业作坊生产的时代。为了完成建模，工程师需要储备大量数学知识，并为特征工程工作积累大量行业知识。每个模型是极其个性化的，建模者如同手工业者一样，将自己的积累形成模型的“个性化签名”。而今，“深度学习工程师”进入了工业化大生产时代。只要掌握深度学习必要但少量的理论知识，掌握Python编程，即可在深度学习框架上实现非常有效的模型，甚至与该领域最领先的模型不相上下。建模这个被“老科学家”们长期把持的建模领域面临着颠覆，也是新入行者的机遇。\n",
    "\n",
    "<center><img src=\"./images/9.png\" width=\"1000\" hegiht=\"\" ></center>\n",
    "<center><br>图14：深度学习工程师处于工业化大生产时代，“老科学家”长期积累的优势不再牢固</br></center>\n",
    "<br></br>\n",
    "\n",
    "人生天地之间，若白驹过隙，忽然而已，每个人都希望留下自己的足迹。为何要学习深度学习技术，以及如何通过这本书来学习呢？一方面，深度学习的应用前景广阔，是极好的发展方向和职业选择。另一方面，本书会使用国产的深度学习框架飞桨（PaddlePaddle）来编写实践案例，基于框架的编程让深度学习变得易学易用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 波士顿房价预测任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集介绍\n",
    "上一节我们初步认识了神经网络的基本概念（如神经元、多层连接、前向计算、计算图）和模型结构三要素（模型假设、评价函数和优化算法）。本节将以“波士顿房价预测”任务为例，向读者介绍使用Python语言和Numpy库来构建神经网络模型的思考过程和操作方法。\n",
    "\n",
    "波士顿房价预测是一个经典的机器学习任务，类似于程序员世界的“Hello World”。和大家对房价的普遍认知相同，波士顿地区的房价受诸多因素影响。该数据集统计了13种可能影响房价的因素和该类型房屋的均价，期望构建一个基于13个因素进行房价预测的模型，如 **图1** 所示。\n",
    "<br></br>\n",
    "<center><img src=\"./images/10.png\" width=\"500\" hegiht=\"\" ></center>\n",
    "<center><br>图1：波士顿房价影响因素示意图</br></center>\n",
    "<br></br>\n",
    "\n",
    "对于预测问题，可以根据预测输出的类型是连续的实数值，还是离散的标签，区分为回归任务和分类任务。因为房价是一个连续值，所以房价预测显然是一个回归任务。下面我们尝试用最简单的线性回归模型解决这个问题，并用神经网络来实现这个模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归模型\n",
    "\n",
    "假设房价和各影响因素之间能够用线性关系来描述：\n",
    "\n",
    "$$y = {\\sum_{j=1}^Mx_j w_j} + b$$\n",
    "\n",
    "模型的求解即是通过数据拟合出每个$w_j$和$b$。其中，$w_j$和$b$分别表示该线性模型的权重和偏置。一维情况下，$w_j$ 和 $b$ 是直线的斜率和截距。\n",
    "\n",
    "线性回归模型使用均方误差作为损失函数（Loss），用以衡量预测房价和真实房价的差异，公式如下：\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat{Y_i} - {Y_i})^{2}$$\n",
    "\n",
    "------\n",
    "**思考：**\n",
    "\n",
    "为什么要以均方误差作为损失函数？即将模型在每个训练样本上的预测误差加和，来衡量整体样本的准确性。这是因为损失函数的设计不仅仅要考虑“合理性”，同样需要考虑“易解性”，这个问题在后面的内容中会详细阐述。\n",
    "\n",
    "------\n",
    "\n",
    "## 线性回归模型的神经网络结构\n",
    "\n",
    "神经网络的标准结构中每个神经元由加权和与非线性变换构成，然后将多个神经元分层的摆放并连接形成神经网络。线性回归模型可以认为是神经网络模型的一种极简特例，是一个只有加权和、没有非线性变换的神经元（无需形成网络），如 **图2** 所示。\n",
    "<center><img src=\"./images/11.png\" width=\"300\" hegiht=\"\" ></center>\n",
    "<center><br>图2：线性回归模型的神经网络结构</br></center>\n",
    "<br></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建波士顿房价预测任务的神经网络模型\n",
    "深度学习不仅实现了模型的端到端学习，还推动了人工智能进入工业大生产阶段，产生了标准化、自动化和模块化的通用框架。不同场景的深度学习模型具备一定的通用性，五个步骤即可完成模型的构建和训练，如 **图3** 所示。\n",
    "<br></br>\n",
    "<center><img src=\"./images/12.png\" width=\"800\" hegiht=\"\" ></center>\n",
    "<center><br>图3：构建神经网络/深度学习模型的基本步骤</br></center>\n",
    "<br></br>\n",
    "\n",
    "正是由于深度学习的建模和训练的过程存在通用性，在构建不同的模型时，只有模型三要素不同，其它步骤基本一致，深度学习框架才有用武之地。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.数据处理\n",
    "数据处理包含五个部分：数据导入、数据形状变换、数据集划分、数据归一化处理和封装`load data`函数。数据预处理后，才能被模型调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datafile):\n",
    "    # 从文件导入数据\n",
    "    # datafile = './data/housing.data'\n",
    "    data = np.fromfile(datafile, sep=' ')\n",
    "\n",
    "    # 每条数据包括14项，其中前面13项是影响因素，第14项是相应的房屋价格中位数\n",
    "    feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \\\n",
    "                      'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\n",
    "    feature_num = len(feature_names)\n",
    "\n",
    "    # 将原始数据进行Reshape，变成[N, 14]这样的形状\n",
    "    data = data.reshape([data.shape[0] // feature_num, feature_num])\n",
    "\n",
    "    # 将原数据集拆分成训练集和测试集\n",
    "    # 这里使用80%的数据做训练，20%的数据做测试\n",
    "    # 测试集和训练集必须是没有交集的\n",
    "    ratio = 0.8\n",
    "    offset = int(data.shape[0] * ratio)\n",
    "    training_data = data[:offset]\n",
    "\n",
    "    # 计算训练集的最大值，最小值，平均值\n",
    "    maximums, minimums, avgs = training_data.max(axis=0), training_data.min(axis=0), \\\n",
    "                                 training_data.sum(axis=0) / training_data.shape[0]\n",
    "\n",
    "    # 对数据进行归一化处理\n",
    "    for i in range(feature_num):\n",
    "        #print(maximums[i], minimums[i], avgs[i])\n",
    "        data[:, i] = (data[:, i] - minimums[i]) / (maximums[i] - minimums[i])\n",
    "\n",
    "    # 训练集和测试集的划分比例\n",
    "    training_data = data[:offset]\n",
    "    test_data = data[offset:]\n",
    "    return training_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, test_data = load_data(\"./data/housing.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.18      , 0.07344184, 0.        , 0.31481481,\n",
       "       0.57750527, 0.64160659, 0.26920314, 0.        , 0.22755741,\n",
       "       0.28723404, 1.        , 0.08967991, 0.42222222])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型设计\n",
    "\n",
    "模型设计是深度学习模型关键要素之一，也称为网络结构设计，相当于模型的假设空间，即实现模型“前向计算”（从输入到输出）的过程。\n",
    "\n",
    "如果将输入特征和输出预测值均以向量表示，输入特征$x$有13个分量，$y$有1个分量，那么参数权重的形状（shape）是$13\\times1$。假设我们以如下任意数字赋值参数做初始化：\n",
    "$$w=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, -0.1, -0.2, -0.3, -0.4, 0.0]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，\n",
    "        # 此处设置固定的随机数种子\n",
    "        np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights, 1)  # 仅仅包含一个列表，因此只需要一列参数\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b  # 矩阵乘法公式\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练配置\n",
    "\n",
    "模型设计完成后，需要通过训练配置寻找模型的最优值，即通过损失函数来衡量模型的好坏。训练配置也是深度学习模型关键要素之一。\n",
    "\n",
    "通过模型计算$x_1$表示的影响因素所对应的房价应该是$z$, 但实际数据告诉我们房价是$y$。这时我们需要有某种指标来衡量预测值$z$跟真实值$y$之间的差距。对于回归问题，最常采用的衡量方法是使用**均方误差**作为评价模型好坏的指标，具体定义如下：\n",
    "\n",
    "$$Loss = (y - z)^2$$\n",
    "\n",
    "上式中的$Loss$（简记为: $L$）通常也被称作损失函数，它是衡量模型好坏的指标。在回归问题中，均方误差是一种比较常见的形式，分类问题中通常会采用**交叉熵作为损失函数**，在后续的章节中会更详细的介绍。对一个样本计算损失函数值的实现如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为计算损失函数时需要把每个样本的损失函数值都考虑到，所以我们需要对单个样本的损失函数进行求和，并除以样本总数$N$。\n",
    "$$Loss= \\frac{1}{N}\\sum_{i=1}^N{(y_i - z_i)^2}$$\n",
    "在Network类下面添加损失函数的计算过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\n",
    "        np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights, 1)\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z\n",
    "    \n",
    "    def loss(self, z, y):\n",
    "        error = z - y\n",
    "        cost = error * error\n",
    "        cost = np.mean(cost)\n",
    "        return cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练过程\n",
    "\n",
    "上述计算过程描述了如何构建神经网络，通过神经网络完成预测值和损失函数的计算。接下来介绍如何求解参数$w$和$b$的数值，这个过程也称为模型训练过程。训练过程是深度学习模型的关键要素之一，其目标是让定义的损失函数$Loss$尽可能的小，也就是说找到一个参数解$w$和$b$，使得损失函数取得极小值。\n",
    "\n",
    "我们先做一个小测试：如 **图5** 所示，基于微积分知识，求一条曲线在某个点的斜率等于函数在该点的导数值。那么大家思考下，当处于曲线的极值点时，该点的斜率是多少？\n",
    "\n",
    "<center><img src=\"./images/13.png\" width=\"300\" hegiht=\"\" ></center>\n",
    "<center><br>图5：曲线斜率等于导数值</br></center>\n",
    "<br></br>\n",
    "\n",
    "这个问题并不难回答，处于曲线极值点时的斜率为0，即函数在极值点的导数为0。那么，让损失函数取极小值的$w$和$b$应该是下述方程组的解：\n",
    "$$\\frac{\\partial{L}}{\\partial{w}}=0$$\n",
    "$$\\frac{\\partial{L}}{\\partial{b}}=0$$\n",
    "\n",
    "将样本数据$(x, y)$带入上面的方程组中即可求解出$w$和$b$的值，但是这种方法只对线性回归这样简单的任务有效。如果模型中含有非线性变换，或者损失函数不是均方差这种简单的形式，则很难通过上式求解。为了解决这个问题，下面我们将引入更加普适的数值求解方法：梯度下降法。\n",
    "\n",
    "### 梯度下降法（沿着函数的梯度方向可以优先找到山谷，即极值点）\n",
    "\n",
    "在现实中存在大量的函数正向求解容易，但反向求解较难，被称为单向函数，这种函数在密码学中有大量的应用。密码锁的特点是可以迅速判断一个密钥是否是正确的(已知$x$，求$y$很容易)，但是即使获取到密码锁系统，无法破解出正确的密钥是什么（已知$y$，求$x$很难）。\n",
    "\n",
    "这种情况特别类似于一位想从山峰走到坡谷的盲人，他看不见坡谷在哪（无法逆向求解出$Loss$导数为0时的参数值），但可以伸脚探索身边的坡度（当前点的导数值，也称为梯度）。那么，求解Loss函数最小值可以这样实现：从当前的参数取值，一步步的按照下坡的方向下降，直到走到最低点。这种方法笔者称它为“盲人下坡法”。哦不，有个更正式的说法“梯度下降法”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算梯度\n",
    "\n",
    "上面我们讲过了损失函数的计算方法，这里稍微改写，为了使梯度计算更加简洁，引入因子$\\frac{1}{2}$，定义损失函数如下：\n",
    "\n",
    "$$L= \\frac{1}{2N}\\sum_{i=1}^N{(y_i - z_i)^2}$$\n",
    "\n",
    "其中$z_i$是网络对第$i$个样本的预测值：\n",
    "\n",
    "$$z_i = \\sum_{j=0}^{12}{x_i^{j}\\cdot w_j} + b$$\n",
    "\n",
    "梯度的定义：\n",
    "\n",
    "$$𝑔𝑟𝑎𝑑𝑖𝑒𝑛𝑡 = (\\frac{\\partial{L}}{\\partial{w_0}},\\frac{\\partial{L}}{\\partial{w_1}}, ... ,\\frac{\\partial{L}}{\\partial{w_{12}}} ,\\frac{\\partial{L}}{\\partial{b}})$$\n",
    "\n",
    "可以计算出$L$对$w$和$b$的偏导数：\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{w_j}} = \\frac{1}{N}\\sum_{i=1}^N{(z_i - y_i)\\frac{\\partial{z_i}}{\\partial{w_j}}} = \\frac{1}{N}\\sum_{i=1}^N{(z_i - y_i)x_i^{j}}$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{b}} = \\frac{1}{N}\\sum_{i=1}^N{(z_i - y_i)\\frac{\\partial{z_i}}{\\partial{b}}} = \\frac{1}{N}\\sum_{i=1}^N{(z_i - y_i)}$$\n",
    "\n",
    "从导数的计算过程可以看出，因子$\\frac{1}{2}$被消掉了，这是因为二次函数求导的时候会产生因子$2$，这也是我们将损失函数改写的原因。\n",
    "\n",
    "下面我们考虑只有一个样本的情况下，计算梯度：\n",
    "\n",
    "$$L= \\frac{1}{2}{(y_i - z_i)^2}$$\n",
    "\n",
    "$$z_1 = {x_1^{0}\\cdot w_0} + {x_1^{1}\\cdot w_1} + ...  + {x_1^{12}\\cdot w_{12}} + b$$\n",
    "\n",
    "可以计算出：\n",
    "\n",
    "$$L= \\frac{1}{2}{({x_1^{0}\\cdot w_0} + {x_1^{1}\\cdot w_1} + ...  + {x_1^{12}\\cdot w_{12}} + b - y_1)^2}$$\n",
    "\n",
    "可以计算出$L$对$w$和$b$的偏导数：\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{w_0}} = ({x_1^{0}\\cdot w_0} + {x_1^{1}\\cdot w_1} + ...  + {x_1^{12}\\cdot w_12} + b - y_1)\\cdot x_1^{0}=({z_1} - {y_1})\\cdot x_1^{0}$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{b}} = ({x_1^{0}\\cdot w_0} + {x_1^{1}\\cdot w_1} + ...  + {x_1^{12}\\cdot w_{12}} + b - y_1)\\cdot 1 = ({z_1} - {y_1})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\n",
    "        np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights, 1)\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z\n",
    "    \n",
    "    def loss(self, z, y):\n",
    "        error = z - y\n",
    "        num_samples = error.shape[0]\n",
    "        cost = error * error\n",
    "        cost = np.sum(cost) / num_samples\n",
    "        return cost\n",
    "    \n",
    "    def gradient(self, x, y):\n",
    "        z = self.forward(x)\n",
    "        gradient_w = (z-y)*x\n",
    "        gradient_w = np.mean(gradient_w, axis=0)\n",
    "        # 我们使用Numpy的矩阵操作方便地完成了gradient的计算，\n",
    "        # 但引入了一个问题，gradient_w的形状是(13,)，而www的维度是(13, 1)。\n",
    "        # 导致该问题的原因是使用np.mean函数时消除了第0维。为了加减乘除等计算方便，gradient_w和www必须保持一致的形状。\n",
    "        gradient_w = gradient_w[:, np.newaxis]\n",
    "        # print(gradient_w)\n",
    "        gradient_b = (z - y)\n",
    "        gradient_b = np.mean(gradient_b)\n",
    "        \n",
    "        return gradient_w, gradient_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = training_data[:, :-1]\n",
    "y = training_data[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -2.75916864]\n",
      " [-11.7250307 ]\n",
      " [-32.9933848 ]\n",
      " [ -8.05121635]\n",
      " [-28.32223032]\n",
      " [-45.87968288]\n",
      " [-56.12498734]\n",
      " [-21.62493072]\n",
      " [-27.04237003]\n",
      " [-35.50236884]\n",
      " [-49.71676615]\n",
      " [-80.22146398]\n",
      " [-22.95075827]]\n",
      "point [-100.0, -100.0], loss 7873.345739941161\n",
      "gradient [-45.87968288123223, -35.50236884482904]\n"
     ]
    }
   ],
   "source": [
    "# 调用上面定义的gradient函数，计算梯度\n",
    "# 初始化网络\n",
    "net = Network(13)\n",
    "# 设置[w5, w9] = [-100., -100.]\n",
    "net.w[5] = -100.0\n",
    "net.w[9] = -100.0\n",
    "\n",
    "z = net.forward(x)\n",
    "loss = net.loss(z, y)\n",
    "gradient_w, gradient_b = net.gradient(x, y)\n",
    "gradient_w5 = gradient_w[5][0]\n",
    "gradient_w9 = gradient_w[9][0]\n",
    "print('point {}, loss {}'.format([net.w[5][0], net.w[9][0]], loss))\n",
    "print('gradient {}'.format([gradient_w5, gradient_w9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 确定损失函数更小的点\n",
    "\n",
    "下面我们开始研究更新梯度的方法。首先沿着梯度的反方向移动一小步，找到下一个点P1，观察损失函数的变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -2.64748812]\n",
      " [-11.2092481 ]\n",
      " [-31.59251741]\n",
      " [ -7.69794835]\n",
      " [-27.12066138]\n",
      " [-43.883933  ]\n",
      " [-53.71643128]\n",
      " [-20.67653287]\n",
      " [-25.91922095]\n",
      " [-34.01927391]\n",
      " [-47.58279595]\n",
      " [-76.7478908 ]\n",
      " [-21.97361135]]\n",
      "point [-95.41203171187678, -96.4497631155171], loss 7214.694816482369\n",
      "gradient [-43.883932999069096, -34.019273908495926]\n"
     ]
    }
   ],
   "source": [
    "# 在[w5, w9]平面上，沿着梯度的反方向移动到下一个点P1\n",
    "# 定义移动步长 eta\n",
    "eta = 0.1\n",
    "# 更新参数w5和w9\n",
    "net.w[5] = net.w[5] - eta * gradient_w5\n",
    "net.w[9] = net.w[9] - eta * gradient_w9\n",
    "# 重新计算z和loss\n",
    "z = net.forward(x)\n",
    "loss = net.loss(z, y)\n",
    "gradient_w, gradient_b = net.gradient(x, y)\n",
    "gradient_w5 = gradient_w[5][0]\n",
    "gradient_w9 = gradient_w[9][0]\n",
    "print('point {}, loss {}'.format([net.w[5][0], net.w[9][0]], loss))\n",
    "print('gradient {}'.format([gradient_w5, gradient_w9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行上面的代码，可以发现沿着梯度反方向走一小步，下一个点的损失函数的确减少了。感兴趣的话，大家可以尝试不停的点击上面的代码块，观察损失函数是否一直在变小。\n",
    "\n",
    "在上述代码中，每次更新参数使用的语句：\n",
    "`net.w[5] = net.w[5] - eta * gradient_w5`\n",
    "\n",
    "* 相减：参数需要向梯度的反方向移动。\n",
    "* eta：控制每次参数值沿着梯度反方向变动的大小，即每次移动的步长，又称为学习率。\n",
    "\n",
    "大家可以思考下，为什么之前我们要做输入特征的归一化，保持尺度一致？这是为了让统一的步长更加合适。\n",
    "\n",
    "如 **图8** 所示，特征输入归一化后，不同参数输出的Loss是一个比较规整的曲线，学习率可以设置成统一的值 ；特征输入未归一化时，不同特征对应的参数所需的步长不一致，尺度较大的参数需要大步长，尺寸较小的参数需要小步长，导致无法设置统一的学习率。\n",
    "\n",
    "<center><img src=\"./images/14.png\" width=\"300\" hegiht=\"40\" ></center>\n",
    "<center><br>图8：未归一化的特征，会导致不同特征维度的理想步长不同</br></center>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\n",
    "        np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights, 1)  # 初始化权重, 仅仅包含一个类别, 单条数据包含num_of_weights个特征数据\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        z = np.dot(x, self.w) + self.b \n",
    "        return z\n",
    "    \n",
    "    def loss(self, z, y):\n",
    "        \"\"\"计算损失：loss = (z-y)^2/n\"\"\"\n",
    "        error = z - y  # numpy 广播机制\n",
    "        num_samples = error.shape[0]\n",
    "        cost = error * error\n",
    "        cost = np.sum(cost) / num_samples\n",
    "        return cost\n",
    "    \n",
    "    def gradient(self, x, y):\n",
    "        \"\"\"计算损失函数的梯度\"\"\"\n",
    "        z = self.forward(x)\n",
    "        z = self.forward(x)\n",
    "        gradient_w = (z-y) * x\n",
    "        gradient_w = np.mean(gradient_w, axis=0)\n",
    "        # 我们使用Numpy的矩阵操作方便地完成了gradient的计算，\n",
    "        # 但引入了一个问题，gradient_w的形状是(13,)，而www的维度是(13, 1)。\n",
    "        # 导致该问题的原因是使用np.mean函数时消除了第0维。为了加减乘除等计算方便，gradient_w和www必须保持一致的形状。\n",
    "        gradient_w = gradient_w[:, np.newaxis]\n",
    "        gradient_b = (z - y)\n",
    "        gradient_b = np.mean(gradient_b)        \n",
    "        return gradient_w, gradient_b\n",
    "    \n",
    "    def update(self, gradient_w, gradient_b, eta = 0.01):\n",
    "        \"\"\"参数更新: w = w - eta * gradient_w\n",
    "                     b = b - eta * gradient_b\"\"\"\n",
    "        self.w = self.w - eta * gradient_w\n",
    "        self.b = self.b - eta * gradient_b\n",
    "    \n",
    "    def train(self, x, y, iterations=100, eta=0.01):\n",
    "        losses = []\n",
    "        for i in range(iterations):\n",
    "            z = self.forward(x)\n",
    "            L = self.loss(z, y)\n",
    "            gradient_w, gradient_b = self.gradient(x, y)\n",
    "            self.update(gradient_w, gradient_b, eta)\n",
    "            losses.append(L)\n",
    "            if (i+1) % 10 == 0:\n",
    "                print('iter {}, loss {}'.format(i, L))\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 9, loss 5.143394325795511\n",
      "iter 19, loss 3.097924194225988\n",
      "iter 29, loss 2.082241020617026\n",
      "iter 39, loss 1.5673801618157397\n",
      "iter 49, loss 1.2966204735077431\n",
      "iter 59, loss 1.1453399043319765\n",
      "iter 69, loss 1.0530155717435201\n",
      "iter 79, loss 0.9902292156463155\n",
      "iter 89, loss 0.9426576903842504\n",
      "iter 99, loss 0.9033048096880774\n",
      "iter 109, loss 0.868732003041364\n",
      "iter 119, loss 0.837229250968144\n",
      "iter 129, loss 0.807927474161227\n",
      "iter 139, loss 0.7803677341465797\n",
      "iter 149, loss 0.7542920908532763\n",
      "iter 159, loss 0.7295420168915829\n",
      "iter 169, loss 0.7060090054240882\n",
      "iter 179, loss 0.6836105084697767\n",
      "iter 189, loss 0.6622781710179412\n",
      "iter 199, loss 0.6419520361168637\n",
      "iter 209, loss 0.622577651786949\n",
      "iter 219, loss 0.6041045903195837\n",
      "iter 229, loss 0.5864856570315078\n",
      "iter 239, loss 0.5696764374763879\n",
      "iter 249, loss 0.5536350125932015\n",
      "iter 259, loss 0.5383217588525027\n",
      "iter 269, loss 0.5236991929680567\n",
      "iter 279, loss 0.509731841376165\n",
      "iter 289, loss 0.4963861247069634\n",
      "iter 299, loss 0.48363025234390233\n",
      "iter 309, loss 0.47143412454019784\n",
      "iter 319, loss 0.45976924072044867\n",
      "iter 329, loss 0.44860861316590983\n",
      "iter 339, loss 0.4379266855659793\n",
      "iter 349, loss 0.4276992560632111\n",
      "iter 359, loss 0.4179034044959738\n",
      "iter 369, loss 0.4085174235863553\n",
      "iter 379, loss 0.39952075384787633\n",
      "iter 389, loss 0.39089392200622347\n",
      "iter 399, loss 0.382618482740513\n",
      "iter 409, loss 0.3746769635645124\n",
      "iter 419, loss 0.36705281267772816\n",
      "iter 429, loss 0.35973034962581096\n",
      "iter 439, loss 0.35269471861856694\n",
      "iter 449, loss 0.3459318443621334\n",
      "iter 459, loss 0.33942839026966587\n",
      "iter 469, loss 0.33317171892221653\n",
      "iter 479, loss 0.3271498546584252\n",
      "iter 489, loss 0.3213514481781961\n",
      "iter 499, loss 0.31576574305173283\n",
      "iter 509, loss 0.3103825440311682\n",
      "iter 519, loss 0.30519218706757245\n",
      "iter 529, loss 0.30018551094136725\n",
      "iter 539, loss 0.29535383041913843\n",
      "iter 549, loss 0.29068891085453674\n",
      "iter 559, loss 0.28618294415539336\n",
      "iter 569, loss 0.28182852604338504\n",
      "iter 579, loss 0.27761863453655344\n",
      "iter 589, loss 0.27354660958874766\n",
      "iter 599, loss 0.2696061338236152\n",
      "iter 609, loss 0.26579121430413205\n",
      "iter 619, loss 0.26209616528184804\n",
      "iter 629, loss 0.25851559187303397\n",
      "iter 639, loss 0.25504437461176843\n",
      "iter 649, loss 0.2516776548326958\n",
      "iter 659, loss 0.2484108208387405\n",
      "iter 669, loss 0.24523949481147198\n",
      "iter 679, loss 0.24215952042409844\n",
      "iter 689, loss 0.2391669511192288\n",
      "iter 699, loss 0.2362580390155805\n",
      "iter 709, loss 0.2334292244097483\n",
      "iter 719, loss 0.2306771258409729\n",
      "iter 729, loss 0.22799853068858245\n",
      "iter 739, loss 0.22539038627340982\n",
      "iter 749, loss 0.22284979143604464\n",
      "iter 759, loss 0.22037398856623477\n",
      "iter 769, loss 0.21796035605914357\n",
      "iter 779, loss 0.2156064011754777\n",
      "iter 789, loss 0.21330975328373866\n",
      "iter 799, loss 0.2110681574640261\n",
      "iter 809, loss 0.20887946845393043\n",
      "iter 819, loss 0.20674164491810018\n",
      "iter 829, loss 0.2046527440240648\n",
      "iter 839, loss 0.20261091630783168\n",
      "iter 849, loss 0.20061440081366638\n",
      "iter 859, loss 0.1986615204933024\n",
      "iter 869, loss 0.19675067785062839\n",
      "iter 879, loss 0.19488035081864621\n",
      "iter 889, loss 0.19304908885621125\n",
      "iter 899, loss 0.1912555092527351\n",
      "iter 909, loss 0.1894982936296714\n",
      "iter 919, loss 0.18777618462820625\n",
      "iter 929, loss 0.18608798277314595\n",
      "iter 939, loss 0.18443254350353405\n",
      "iter 949, loss 0.18280877436103968\n",
      "iter 959, loss 0.18121563232764165\n",
      "iter 969, loss 0.17965212130459232\n",
      "iter 979, loss 0.1781172897250724\n",
      "iter 989, loss 0.1766102282933619\n",
      "iter 999, loss 0.17513006784373505\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAXY0lEQVR4nO3de4xc53nf8e8zZ647eyV3eREpaUVLpq3YaUStAzlO1MQ24EuMqG3SQgmcxEkboZe0jtsisBEUQf8pmsIwnKCGA0F20NiujVQ2YsNQErdJLNtNKnt1iSWFkmiREu/irLnk3mfn8vSPc2Y5w12KQ3KH887M7wMMzplzzsw+7y7523efOXPG3B0REQlXqtsFiIjI61NQi4gETkEtIhI4BbWISOAU1CIigUt34kknJyd9enq6E08tItKXnnzyyTl3n9pqX0eCenp6mtnZ2U48tYhIXzKzV6+0T60PEZHAKahFRAKnoBYRCZyCWkQkcApqEZHAKahFRAKnoBYRCVxQQf0Hf3mEx18qdbsMEZGgBBXUf/j4y3xbQS0i0iKooM6mU6zX6t0uQ0QkKGEFdZRivaqgFhFpFlZQpxXUIiKXCy6oy2p9iIi0CCuo1foQEdkkrKBW60NEZJOwgjpKUVHrQ0SkRVhBrRm1iMgm4QW1ZtQiIi3CCmq9mCgisklYQa3Wh4jIJsEFdVlBLSLSIqigzqlHLSKySVtBbWYfMbPnzew5M/uimeU7UYx61CIim101qM1sH/DvgBl3fwsQAQ92ohj1qEVENmu39ZEGCmaWBoaA050oRqfniYhsdtWgdvdTwMeB48AZ4KK7f+Py48zsITObNbPZUun6Lv6fiVLU6k6t7tf1eBGRftRO62MCeAC4A7gFKJrZBy8/zt0fdvcZd5+Zmpq6rmKy6bgctT9ERC5pp/XxbuCYu5fcvQJ8BfiJThSTjRTUIiKXayeojwP3mdmQmRnwLuBwJ4rJNWbU6lOLiGxop0f9BPAo8BTwbPKYhztRTFZBLSKySbqdg9z9d4Hf7XAt6lGLiGwhqHcmZqMIUFCLiDQLK6g1oxYR2STMoK7VulyJiEg4wgrq5PQ8XUFPROSSsIJarQ8RkU2CCuqcglpEZJOggjoT6TxqEZHLBRXUan2IiGymoBYRCVxYQZ20PipqfYiIbAgrqNM6PU9E5HJBBXVOQS0isomCWkQkcEEFtZmRS6coV/QWchGRhqCCGiCfiVhTUIuIbAguqHPplFofIiJNggtqzahFRFoFF9SaUYuItAouqDWjFhFpFWBQp1iraEYtItIQXFDn0hHlqmbUIiINwQW1ZtQiIq2CC2rNqEVEWoUX1JpRi4i0CC+o05FOzxMRaRJcUOczutaHiEizAIM6Yk09ahGRDcEFdS6dolJzanXvdikiIkEILqjzmQhAZ36IiCSCC+qNDw/QmR8iIkCAQd2YUatPLSISCzCo45J0LrWISCy4oM6l1aMWEWkWXFBrRi0i0iq4oN6YUetNLyIiQIBBvTGj1tvIRUSAAINaM2oRkVZtBbWZjZvZo2b2gpkdNrO3d6ogzahFRFql2zzu94E/d/dfMLMsMNSpghoz6rV1zahFRKCNoDazUeB+4EMA7r4OrHeqoKFsHNSran2IiADttT4OACXgj8zsaTN7xMyKnSqooKAWEWnRTlCngUPAp939HmAZ+OjlB5nZQ2Y2a2azpVLpugvKJ62PFbU+RESA9oL6JHDS3Z9I7j9KHNwt3P1hd59x95mpqanrLyhl5DMpVter1/0cIiL95KpB7e5ngRNmdjDZ9C7g7ztZ1FA2rdaHiEii3bM+/i3wheSMj6PAr3WuJChkIrU+REQSbQW1uz8DzHS4lg2FbMSqglpEBAjwnYkQn6Kn1oeISCzIoM6r9SEisiHIoB7KRqxpRi0iAgQc1JpRi4jEggzqfEYvJoqINAQZ1HoxUUTkkkCDOs2K3pkoIgIEGtT5TMRapU697t0uRUSk64IM6salTtf0SeQiImEGdSGjK+iJiDSEGdSNa1IrqEVEwgxqfcqLiMglQQa1Wh8iIpeEGdRqfYiIbAgyqIey8dVXdS61iEigQT2ci2fUS2UFtYhIkEFdzMUz6uWyWh8iIoEHtWbUIiJhBnXSo1brQ0Qk0KCOUsZQNtKMWkSEQIMa4vaHZtQiIgEH9bCCWkQECDioizm1PkREIOSgzqZ1ep6ICAEHtVofIiKxYIO6mEuzrLeQi4iEG9TD+TRLawpqEZFwg1qtDxERIOCgLmbTlKt1qrV6t0sREemqcIM6uYKezvwQkUEXbFAPJxdmWtILiiIy4IINal1BT0QkFmxQj+TjoF5cq3S5EhGR7go2qEcLGQAWVjWjFpHBFm5Q55Og1oxaRAZcsEE9tjGjVlCLyGALNqgbPeqLCmoRGXDBBnU+E5FLp1jQ28hFZMC1HdRmFpnZ02b29U4W1Gy0kFHrQ0QG3rXMqD8MHO5UIVsZK2T0YqKIDLy2gtrM9gM/CzzS2XJajebT6lGLyMBrd0b9SeC3gSteIcnMHjKzWTObLZVK21Jc3PpQj1pEBttVg9rMPgCcc/cnX+84d3/Y3WfcfWZqampbihvNq/UhItLOjPodwM+Z2SvAl4B3mtnnO1pVYqyQUetDRAbeVYPa3T/m7vvdfRp4EPgrd/9gxysDRgtpFlYruPvN+HIiIkEK9jxqiFsfdYfldV2TWkQG1zUFtbt/090/0KliLtd4G/mFlfWb9SVFRIIT9Ix6fCgLwIUV9alFZHAFHdQ7h+OgPr+sGbWIDK6gg3oimVHPq/UhIgMs6KDeUdSMWkQk6KAeK2Qwg3kFtYgMsKCDOkoZ44UM59X6EJEBFnRQA0wUs8wv66wPERlcwQf1zmJWPWoRGWjBB/XEUFZnfYjIQAs+qHdoRi0iAy74oJ4oxjNqXZhJRAZV8EG9s5ilUnN9gICIDKzgg3pqJAdAaWmty5WIiHRH8EG9ayQPwLmFcpcrERHpjvCDejSeUZ9bVFCLyGAKPqgbrY9zi2p9iMhgCj6oR3Jp8pmUWh8iMrCCD2ozY9dIXq0PERlYwQc1wK6RHCUFtYgMqN4I6tGcetQiMrB6I6hH8upRi8jA6omg3juWZ7FcZWFNlzsVkcHTE0G9b6IAwOkLq12uRETk5uuJoL5lPA7qU/MKahEZPD0R1PvHNaMWkcHVE0E9OZwjG6U4qaAWkQHUE0GdShl7x/OcvqBT9ERk8PREUAPsGy9wan6l22WIiNx0PRPU+ycKnNCLiSIygHomqKcni5QWyyyV9UkvIjJYeiaoD0wWAXhlbrnLlYiI3Fw9E9TTSVAfVVCLyIDpnaDeqRm1iAymngnqfCbilrE8xxTUIjJgeiaoAQ5MDfNyaanbZYiI3FQ9FdQH94zw0muL1Ore7VJERG6angrqN+0ZYa1S59Ufqv0hIoPjqkFtZrea2V+b2WEze97MPnwzCtvKm/eOAnD4zGK3ShARuenamVFXgf/g7m8G7gP+jZnd3dmytnbnrmGilPHC2YVufHkRka64alC7+xl3fypZXwQOA/s6XdhW8pmIN0wVef60glpEBsc19ajNbBq4B3hii30Pmdmsmc2WSqXtqW4L/2D/OM+cuIC7XlAUkcHQdlCb2TDwZeC33H3TlNbdH3b3GXefmZqa2s4aWxy6fYLzy+u8+kNdSU9EBkNbQW1mGeKQ/oK7f6WzJb2+e24bB+DpE/PdLENE5KZp56wPAz4DHHb3T3S+pNd3164RhnNpvveKglpEBkM7M+p3AL8MvNPMnklu7+9wXVcUpYz7DuzgO0fmulWCiMhNlb7aAe7+HcBuQi1t+4dvnOL/HD7HK3PLG1fVExHpVz31zsSG+98Yv1j5+EudO7tERCQUPRnUt+8scvvOIQW1iAyEngxqgHe/eTffOTLHxZVKt0sREemong3qf3zPPtZrdb7+7OlulyIi0lE9G9Q/cssod+4a5k+fPtXtUkREOqpng9rM+CeH9vG9V+Z1kSYR6Ws9G9QAv/i22xjKRjz8+NFulyIi0jE9HdQTxSwPvu02vvZ3pzlxXtf+EJH+1NNBDfAb999BOjL+y2OHu12KiEhH9HxQ7x0r8Js/cyd/9txZvvniuW6XIyKy7Xo+qAF+4/4D3LlrmP/4v77PucW1bpcjIrKt+iKoc+mIT/3SIZbKFf7V559idb3W7ZJERLZNXwQ1wME9I3zin/0YTx+f56HPzbJUrna7JBGRbdE3QQ3w/rfu5fd+/kf5m5d/yC98+m90JoiI9IW+CmqAfzpzK5/90Ns4Nb/Kez75LT73t69Qq+vzFUWkd/VdUEN8veo//8j93Hv7BP/pq8/z3k9+i8eePUNdgS0iPagvgxpg33iBP/71H+dTv3SIujv/+gtP8dMf/yZ/+PjLzC2Vu12eiEjbzH37Z5kzMzM+Ozu77c97vWp157Fnz/C5//cq3z12npTBfQd28r637uU9P7KbXSP5bpcoIgPOzJ5095kt9w1CUDc78toiX33mNI89d4ajpWUgvhLfT941yU/dOcXM9AT5TNTlKkVk0Ciot+DuHDm3xDeeP8u3j8zx1PF5KjUnl05x7+0TzNw+wb3TO7jntnFG85lulysifU5B3YblcpUnjv2Qbx+Z47vHznP4zAJ1BzM4uHuEmekJDt02wVv3jXFgapgoFdTn/YpIj1NQX4elcpVnjl9g9tXzPPnqPE8fv7DxJpqhbMTde0d5y74x3rpvjB/dr/AWkRujoN4GtbrzcmmJZ09e5NlTF3nu1EWeP73AaiV+u/pQNuLgnhHetGeEg7tHeGOy3Dmc63LlItILFNQdUqs7R0tLfD8J7xfOLvDi2UXmmz5wd3I4x8E9wxzcPcrBPcPcuWuEA5NFJorZLlYuIqF5vaBO3+xi+kmUMu7aPcJdu0f4+Xv3A/GLlKWlMi+eXdy4vfTaIl/87vGN2TfA+FCGOyaLHJgc5sBUkTsmL9101omINFNQbzMzY9dInl0jeX7qrqmN7fW6c/z8CkfnljhaWubo3DLHSsv83x/M8eWnTrY8xy1jeW7dMRTfJoa4dUeB/cly90ielHrhIgNFQX2TpFLG9GSR6cki73xT677lcpVjc8stt+PnV/j2kRKvLbS+izIbpdg3UWD/RBze+ycK7B3Ls2csz57ReDmU1Y9VpJ/of3QAirk0b9k3xlv2jW3at1apcerCKifnVzlxfoUT8yucnF/l5PkV/uL0Wc4vr296zGg+zd6xQkt4N25TwzmmRnLsLGZJR317BQGRvqKgDlw+E/GGqWHeMDW85f7V9RpnF9Y4c3GV1xbWOHNxjbON28Iah88sUFoqs9VrxhNDGSaHc/FtJMfkcJbJ4RxTwzkmR7Ib+3YUs+qbi3SRgrrHFbLRxouQV1Kp1Tm3WObsxVVKi2VKS+vMLZaZW2rc1vn+yQvMLZZZvsKn4+QzKSaGsowPZZkYyiTrrcuJYibZHx8zms+ony6yDRTUAyATpdg3XmDfeOGqx66u15hbKlNaKidhvs78yjoXVtaZX6lsLA+fXeBCcv9KV481g+FsmtFChpF8mpF8mtF8Yz3DaCFeNm8fLWQYzV/aXshEmCnsZbApqKVFIRttnHHSjnrdWVyrMr/SCPRKsl7h4mqFxbUKC6vVeLlW4ezCGkfOVVlYq7C4Vr3qhzqYwVAmophLU8ylGcpGFLNpirmIoVyaYjZiKJtmOJdmKNfYl2xPlsVcHPj5TBQvsymyUUq/AKRnKKjlhqRSxthQhrGhDNNcuf2yFXdnZb3G4tqlIF9Yq7KwWkm2VVldr7K8XmO5HC9XylWWylXmltZZPr/CSrmxr3rFmf1WzKDQCO5MRD6T2gjyQjYil46X+XSKQjbenmvsbxybjchGKXKZFNkoSpYpsukUuXS8jNej+H6UUitIrouCWrrGzDZmynvGbuya4O5OuVqPQ7tcY3m9ysp6sl6uslqpsVapJ8v4trpe29i+sa1S2/hF0Lxtdb1GuVq/4TFnIkvCPbpCqKfINoI9nSKX/CLIRCnSqRSZtJFJJfeT50pHRiZKkUmW6ShFtmk9s7E/RTplZNPxsrEtE1nymPi50inTXxuBUVBLXzCzZGYcsXPrE2RuWL0e/zJYbQrw9WqdcrWeLOP7LdtqdcqVGuu1+lWPXa/VKVfqXFyttBxTrtap1OpUa856LV7vwJUfWmyE/kawp4hSRjqyeJkyolS8vxHu8fZU0/7G8amm/Y3jLzuu8Xwtz984/vJjm+5v+tqQsvh+ypKvb0YqFS+jVOt6lGrs59Jjku0h/bJSUIu0KZWyuA2S7f6pirW6U6ldCvBKrU6l7lSqdar1OutVp1qvJ8d4S9BvHJ/si4+vU00eX6k3jr/02EqtTq0OtXp8XPz1veV+te6sVmrJ/fjr1Jr2VWutx8bL+sZzhcaMTSHfuMW/DJr2J7fJYo4/+Zdv3/ZaFNQiPSgOhqivzm+vNwV4pV6nVtsc6Bv3a5uDvlZ3au7U663r1bpT90u/NOL1+JdOfFz8tWvJMZses/GcUPfG17z0mMayWndGcp2J1Lae1czeC/w+EAGPuPt/7Ug1IjKwUikjm7zYWqB/fgFth6u+h9jMIuBTwPuAu4FfNLO7O12YiIjE2rnYw48DP3D3o+6+DnwJeKCzZYmISEM7Qb0PONF0/2SyrYWZPWRms2Y2WyqVtqs+EZGB105Qb3WOyqaXaN39YXefcfeZqampLR4iIiLXo52gPgnc2nR/P3C6M+WIiMjl2gnq7wF3mdkdZpYFHgS+1tmyRESk4aqn57l71cx+E/gL4tPzPuvuz3e8MhERAdo8j9rdHwMe63AtIiKyBfMOXDTAzErAq9f58ElgbhvL6QUa82DQmPvfjYz3dnff8kyMjgT1jTCzWXef6XYdN5PGPBg05v7XqfHq001FRAKnoBYRCVyIQf1wtwvoAo15MGjM/a8j4w2uRy0iIq1CnFGLiEgTBbWISOCCCWoze6+ZvWhmPzCzj3a7nu1iZrea2V+b2WEze97MPpxs32Fm/9vMjiTLiabHfCz5PrxoZu/pXvU3xswiM3vazL6e3O/rMZvZuJk9amYvJD/vtw/AmD+S/Lt+zsy+aGb5fhuzmX3WzM6Z2XNN2655jGZ2r5k9m+z7A7uWD2V0967fiN+a/jJwAMgCfwfc3e26tmlse4FDyfoI8BLxBzD8N+CjyfaPAr+XrN+djD8H3JF8X6Juj+M6x/7vgf8JfD2539djBv4H8C+S9Sww3s9jJr7c8TGgkNz/E+BD/TZm4H7gEPBc07ZrHiPwXeDtxFck/TPgfe3WEMqMum8/nMDdz7j7U8n6InCY+B/4A8T/sUmW/yhZfwD4kruX3f0Y8APi709PMbP9wM8CjzRt7tsxm9ko8X/ozwC4+7q7X6CPx5xIAwUzSwNDxFfW7Ksxu/u3gPOXbb6mMZrZXmDU3f/W49T+46bHXFUoQd3WhxP0OjObBu4BngB2u/sZiMMc2JUc1i/fi08Cvw3Um7b185gPACXgj5J2zyNmVqSPx+zup4CPA8eBM8BFd/8GfTzmJtc6xn3J+uXb2xJKULf14QS9zMyGgS8Dv+XuC6936Bbbeup7YWYfAM65+5PtPmSLbT01ZuKZ5SHg0+5+D7BM/CfxlfT8mJO+7APEf+LfAhTN7IOv95AttvXUmNtwpTHe0NhDCeq+/nACM8sQh/QX3P0ryebXkj+HSJbnku398L14B/BzZvYKcRvrnWb2efp7zCeBk+7+RHL/UeLg7ucxvxs45u4ld68AXwF+gv4ec8O1jvFksn759raEEtR9++EEySu7nwEOu/snmnZ9DfjVZP1Xga82bX/QzHJmdgdwF/GLED3D3T/m7vvdfZr4Z/lX7v5B+nvMZ4ETZnYw2fQu4O/p4zETtzzuM7Oh5N/5u4hfg+nnMTdc0xiT9siimd2XfK9+pekxV9ftV1SbXkV9P/EZES8Dv9PterZxXD9J/CfO94Fnktv7gZ3AXwJHkuWOpsf8TvJ9eJFreGU4xBvw01w666Ovxwz8GDCb/Kz/FJgYgDH/Z+AF4Dngc8RnO/TVmIEvEvfgK8Qz439+PWMEZpLv08vAfyd5Z3g7N72FXEQkcKG0PkRE5AoU1CIigVNQi4gETkEtIhI4BbWISOAU1CIigVNQi4gE7v8DbdISbhGM+J0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 获取数据\n",
    "train_data, test_data = load_data(\"./data/housing.data\")\n",
    "x = train_data[:, :-1]\n",
    "y = train_data[:, -1:]\n",
    "# 创建网络\n",
    "net = Network(13)\n",
    "num_iterations=1000\n",
    "# 启动训练\n",
    "losses = net.train(x,y, iterations=num_iterations, eta=0.01)\n",
    "\n",
    "# 画出损失函数的变化趋势\n",
    "plot_x = np.arange(num_iterations)\n",
    "plot_y = np.array(losses)\n",
    "plt.plot(plot_x, plot_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机梯度下降法（ Stochastic Gradient Descent）\n",
    "\n",
    "在上述程序中，每次损失函数和梯度计算都是基于数据集中的全量数据。对于波士顿房价预测任务数据集而言，样本数比较少，只有404个。但在实际问题中，数据集往往非常大，如果每次都使用全量数据进行计算，效率非常低，通俗地说就是“杀鸡焉用牛刀”。由于参数每次只沿着梯度反方向更新一点点，因此方向并不需要那么精确。一个合理的解决方案是每次从总的数据集中随机抽取出小部分数据来代表整体，基于这部分数据计算梯度和损失来更新参数，这种方法被称作随机梯度下降法（Stochastic Gradient Descent，SGD），核心概念如下：\n",
    "\n",
    "* mini-batch：每次迭代时抽取出来的一批数据被称为一个mini-batch。\n",
    "* batch_size：一个mini-batch所包含的样本数目称为batch_size。\n",
    "* epoch：当程序迭代的时候，按mini-batch逐渐抽取出样本，当把整个数据集都遍历到了的时候，则完成了一轮训练，也叫一个epoch。启动训练时，可以将训练的轮数num_epochs和batch_size作为参数传入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外，这里是按顺序读取mini_batch，而SGD里面是随机抽取一部分样本代表总体。为了实现随机抽样的效果，我们先将train_data里面的样本顺序随机打乱，然后再抽取mini_batch。随机打乱样本顺序，需要用到`np.random.shuffle`函数，下面先介绍它的用法。\n",
    "\n",
    "------\n",
    "**说明：**\n",
    "\n",
    "通过大量实验发现，模型对最后出现的数据印象更加深刻。训练数据导入后，越接近模型训练结束，最后几个批次数据对模型参数的影响越大。为了避免模型记忆影响训练效果，需要进行样本乱序操作。\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **训练过程代码修改**\n",
    "\n",
    "将每个随机抽取的mini-batch数据输入到模型中用于参数训练。训练过程的核心是两层循环：\n",
    "\n",
    "1. 第一层循环，代表样本集合要被训练遍历几次，称为“epoch”，代码如下：\n",
    "\n",
    "`for epoch_id in range(num_epochs):`\n",
    "\n",
    "2. 第二层循环，代表每次遍历时，样本集合被拆分成的多个批次，需要全部执行训练，称为“iter (iteration)”，代码如下：\n",
    "\n",
    "`for iter_id,mini_batch in emumerate(mini_batches):`\n",
    "\n",
    "在两层循环的内部是经典的四步训练流程：前向计算->计算损失->计算梯度->更新参数，这与大家之前所学是一致的，代码如下：\n",
    "\n",
    "                x = mini_batch[:, :-1]\n",
    "                y = mini_batch[:, -1:]\n",
    "                a = self.forward(x)  #前向计算\n",
    "                loss = self.loss(a, y)  #计算损失\n",
    "                gradient_w, gradient_b = self.gradient(x, y)  #计算梯度\n",
    "                self.update(gradient_w, gradient_b, eta)  #更新参数\n",
    "\n",
    "\n",
    "将两部分改写的代码集成到Network类中的`train`函数中，最终的实现如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\n",
    "        #np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights, 1)\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z\n",
    "    \n",
    "    def loss(self, z, y):\n",
    "        error = z - y\n",
    "        num_samples = error.shape[0]\n",
    "        cost = error * error\n",
    "        cost = np.sum(cost) / num_samples\n",
    "        return cost\n",
    "    \n",
    "    def gradient(self, x, y):\n",
    "        z = self.forward(x)\n",
    "        N = x.shape[0]\n",
    "        gradient_w = 1. / N * np.sum((z-y) * x, axis=0)\n",
    "        gradient_w = gradient_w[:, np.newaxis]\n",
    "        gradient_b = 1. / N * np.sum(z-y)\n",
    "        return gradient_w, gradient_b\n",
    "    \n",
    "    def update(self, gradient_w, gradient_b, eta = 0.01):\n",
    "        self.w = self.w - eta * gradient_w\n",
    "        self.b = self.b - eta * gradient_b\n",
    "            \n",
    "                \n",
    "    def train(self, training_data, num_epochs, batch_size=10, eta=0.01):\n",
    "        n = len(training_data)\n",
    "        losses = []\n",
    "        for epoch_id in range(num_epochs):\n",
    "            # 在每轮迭代开始之前，将训练数据的顺序随机打乱\n",
    "            # 然后再按每次取batch_size条数据的方式取出\n",
    "            np.random.shuffle(training_data)\n",
    "            # 将训练数据进行拆分，每个mini_batch包含batch_size条的数据\n",
    "            mini_batches = [training_data[k:k+batch_size] for k in range(0, n, batch_size)]\n",
    "            for iter_id, mini_batch in enumerate(mini_batches):\n",
    "                #print(self.w.shape)\n",
    "                #print(self.b)\n",
    "                x = mini_batch[:, :-1]\n",
    "                y = mini_batch[:, -1:]\n",
    "                z = self.forward(x)\n",
    "                loss = self.loss(z, y)\n",
    "                gradient_w, gradient_b = self.gradient(x, y)  # 计算梯度\n",
    "                self.update(gradient_w, gradient_b, eta)  # 参数更新\n",
    "                losses.append(loss)\n",
    "                print('Epoch {:3d} / iter {:3d}, loss = {:.4f}'.\n",
    "                                 format(epoch_id, iter_id, loss))\n",
    "        \n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 / iter   0, loss = 1.0281\n",
      "Epoch   0 / iter   1, loss = 0.5048\n",
      "Epoch   0 / iter   2, loss = 0.6382\n",
      "Epoch   0 / iter   3, loss = 0.5168\n",
      "Epoch   0 / iter   4, loss = 0.1951\n",
      "Epoch   1 / iter   0, loss = 0.6281\n",
      "Epoch   1 / iter   1, loss = 0.4611\n",
      "Epoch   1 / iter   2, loss = 0.4520\n",
      "Epoch   1 / iter   3, loss = 0.3961\n",
      "Epoch   1 / iter   4, loss = 0.1381\n",
      "Epoch   2 / iter   0, loss = 0.5642\n",
      "Epoch   2 / iter   1, loss = 0.4250\n",
      "Epoch   2 / iter   2, loss = 0.4480\n",
      "Epoch   2 / iter   3, loss = 0.3881\n",
      "Epoch   2 / iter   4, loss = 0.1884\n",
      "Epoch   3 / iter   0, loss = 0.3921\n",
      "Epoch   3 / iter   1, loss = 0.5582\n",
      "Epoch   3 / iter   2, loss = 0.3759\n",
      "Epoch   3 / iter   3, loss = 0.3849\n",
      "Epoch   3 / iter   4, loss = 0.1425\n",
      "Epoch   4 / iter   0, loss = 0.3821\n",
      "Epoch   4 / iter   1, loss = 0.4382\n",
      "Epoch   4 / iter   2, loss = 0.3864\n",
      "Epoch   4 / iter   3, loss = 0.4314\n",
      "Epoch   4 / iter   4, loss = 0.0471\n",
      "Epoch   5 / iter   0, loss = 0.4264\n",
      "Epoch   5 / iter   1, loss = 0.3829\n",
      "Epoch   5 / iter   2, loss = 0.3179\n",
      "Epoch   5 / iter   3, loss = 0.4149\n",
      "Epoch   5 / iter   4, loss = 0.1581\n",
      "Epoch   6 / iter   0, loss = 0.3148\n",
      "Epoch   6 / iter   1, loss = 0.3532\n",
      "Epoch   6 / iter   2, loss = 0.4195\n",
      "Epoch   6 / iter   3, loss = 0.3272\n",
      "Epoch   6 / iter   4, loss = 1.2465\n",
      "Epoch   7 / iter   0, loss = 0.3166\n",
      "Epoch   7 / iter   1, loss = 0.2810\n",
      "Epoch   7 / iter   2, loss = 0.4126\n",
      "Epoch   7 / iter   3, loss = 0.3309\n",
      "Epoch   7 / iter   4, loss = 0.2255\n",
      "Epoch   8 / iter   0, loss = 0.2555\n",
      "Epoch   8 / iter   1, loss = 0.3678\n",
      "Epoch   8 / iter   2, loss = 0.3342\n",
      "Epoch   8 / iter   3, loss = 0.3806\n",
      "Epoch   8 / iter   4, loss = 0.0570\n",
      "Epoch   9 / iter   0, loss = 0.3532\n",
      "Epoch   9 / iter   1, loss = 0.3973\n",
      "Epoch   9 / iter   2, loss = 0.1945\n",
      "Epoch   9 / iter   3, loss = 0.2839\n",
      "Epoch   9 / iter   4, loss = 0.1604\n",
      "Epoch  10 / iter   0, loss = 0.3414\n",
      "Epoch  10 / iter   1, loss = 0.2774\n",
      "Epoch  10 / iter   2, loss = 0.3439\n",
      "Epoch  10 / iter   3, loss = 0.2103\n",
      "Epoch  10 / iter   4, loss = 0.0959\n",
      "Epoch  11 / iter   0, loss = 0.3004\n",
      "Epoch  11 / iter   1, loss = 0.2497\n",
      "Epoch  11 / iter   2, loss = 0.2827\n",
      "Epoch  11 / iter   3, loss = 0.2987\n",
      "Epoch  11 / iter   4, loss = 0.0316\n",
      "Epoch  12 / iter   0, loss = 0.2509\n",
      "Epoch  12 / iter   1, loss = 0.2535\n",
      "Epoch  12 / iter   2, loss = 0.2944\n",
      "Epoch  12 / iter   3, loss = 0.2889\n",
      "Epoch  12 / iter   4, loss = 0.0547\n",
      "Epoch  13 / iter   0, loss = 0.2792\n",
      "Epoch  13 / iter   1, loss = 0.2137\n",
      "Epoch  13 / iter   2, loss = 0.2427\n",
      "Epoch  13 / iter   3, loss = 0.2986\n",
      "Epoch  13 / iter   4, loss = 0.3861\n",
      "Epoch  14 / iter   0, loss = 0.3261\n",
      "Epoch  14 / iter   1, loss = 0.2123\n",
      "Epoch  14 / iter   2, loss = 0.1837\n",
      "Epoch  14 / iter   3, loss = 0.2968\n",
      "Epoch  14 / iter   4, loss = 0.0620\n",
      "Epoch  15 / iter   0, loss = 0.2402\n",
      "Epoch  15 / iter   1, loss = 0.2823\n",
      "Epoch  15 / iter   2, loss = 0.2574\n",
      "Epoch  15 / iter   3, loss = 0.1833\n",
      "Epoch  15 / iter   4, loss = 0.0637\n",
      "Epoch  16 / iter   0, loss = 0.1889\n",
      "Epoch  16 / iter   1, loss = 0.1998\n",
      "Epoch  16 / iter   2, loss = 0.2031\n",
      "Epoch  16 / iter   3, loss = 0.3219\n",
      "Epoch  16 / iter   4, loss = 0.1373\n",
      "Epoch  17 / iter   0, loss = 0.2042\n",
      "Epoch  17 / iter   1, loss = 0.2070\n",
      "Epoch  17 / iter   2, loss = 0.2651\n",
      "Epoch  17 / iter   3, loss = 0.2137\n",
      "Epoch  17 / iter   4, loss = 0.0138\n",
      "Epoch  18 / iter   0, loss = 0.1794\n",
      "Epoch  18 / iter   1, loss = 0.1575\n",
      "Epoch  18 / iter   2, loss = 0.2554\n",
      "Epoch  18 / iter   3, loss = 0.2531\n",
      "Epoch  18 / iter   4, loss = 0.2192\n",
      "Epoch  19 / iter   0, loss = 0.1779\n",
      "Epoch  19 / iter   1, loss = 0.2072\n",
      "Epoch  19 / iter   2, loss = 0.2140\n",
      "Epoch  19 / iter   3, loss = 0.2513\n",
      "Epoch  19 / iter   4, loss = 0.0673\n",
      "Epoch  20 / iter   0, loss = 0.1634\n",
      "Epoch  20 / iter   1, loss = 0.1887\n",
      "Epoch  20 / iter   2, loss = 0.2515\n",
      "Epoch  20 / iter   3, loss = 0.1924\n",
      "Epoch  20 / iter   4, loss = 0.0926\n",
      "Epoch  21 / iter   0, loss = 0.1583\n",
      "Epoch  21 / iter   1, loss = 0.2319\n",
      "Epoch  21 / iter   2, loss = 0.1550\n",
      "Epoch  21 / iter   3, loss = 0.2092\n",
      "Epoch  21 / iter   4, loss = 0.1959\n",
      "Epoch  22 / iter   0, loss = 0.2414\n",
      "Epoch  22 / iter   1, loss = 0.1522\n",
      "Epoch  22 / iter   2, loss = 0.1719\n",
      "Epoch  22 / iter   3, loss = 0.1829\n",
      "Epoch  22 / iter   4, loss = 0.2748\n",
      "Epoch  23 / iter   0, loss = 0.1861\n",
      "Epoch  23 / iter   1, loss = 0.1830\n",
      "Epoch  23 / iter   2, loss = 0.1606\n",
      "Epoch  23 / iter   3, loss = 0.2351\n",
      "Epoch  23 / iter   4, loss = 0.1479\n",
      "Epoch  24 / iter   0, loss = 0.1678\n",
      "Epoch  24 / iter   1, loss = 0.2080\n",
      "Epoch  24 / iter   2, loss = 0.1471\n",
      "Epoch  24 / iter   3, loss = 0.1747\n",
      "Epoch  24 / iter   4, loss = 0.1607\n",
      "Epoch  25 / iter   0, loss = 0.1162\n",
      "Epoch  25 / iter   1, loss = 0.2067\n",
      "Epoch  25 / iter   2, loss = 0.1692\n",
      "Epoch  25 / iter   3, loss = 0.1757\n",
      "Epoch  25 / iter   4, loss = 0.0125\n",
      "Epoch  26 / iter   0, loss = 0.1707\n",
      "Epoch  26 / iter   1, loss = 0.1898\n",
      "Epoch  26 / iter   2, loss = 0.1409\n",
      "Epoch  26 / iter   3, loss = 0.1501\n",
      "Epoch  26 / iter   4, loss = 0.1002\n",
      "Epoch  27 / iter   0, loss = 0.1590\n",
      "Epoch  27 / iter   1, loss = 0.1801\n",
      "Epoch  27 / iter   2, loss = 0.1578\n",
      "Epoch  27 / iter   3, loss = 0.1257\n",
      "Epoch  27 / iter   4, loss = 0.7750\n",
      "Epoch  28 / iter   0, loss = 0.1573\n",
      "Epoch  28 / iter   1, loss = 0.1224\n",
      "Epoch  28 / iter   2, loss = 0.1353\n",
      "Epoch  28 / iter   3, loss = 0.1862\n",
      "Epoch  28 / iter   4, loss = 0.5305\n",
      "Epoch  29 / iter   0, loss = 0.1981\n",
      "Epoch  29 / iter   1, loss = 0.1114\n",
      "Epoch  29 / iter   2, loss = 0.1414\n",
      "Epoch  29 / iter   3, loss = 0.1856\n",
      "Epoch  29 / iter   4, loss = 0.0268\n",
      "Epoch  30 / iter   0, loss = 0.0984\n",
      "Epoch  30 / iter   1, loss = 0.1528\n",
      "Epoch  30 / iter   2, loss = 0.1637\n",
      "Epoch  30 / iter   3, loss = 0.1532\n",
      "Epoch  30 / iter   4, loss = 0.0846\n",
      "Epoch  31 / iter   0, loss = 0.1433\n",
      "Epoch  31 / iter   1, loss = 0.1643\n",
      "Epoch  31 / iter   2, loss = 0.1202\n",
      "Epoch  31 / iter   3, loss = 0.1215\n",
      "Epoch  31 / iter   4, loss = 0.2182\n",
      "Epoch  32 / iter   0, loss = 0.1567\n",
      "Epoch  32 / iter   1, loss = 0.1420\n",
      "Epoch  32 / iter   2, loss = 0.1073\n",
      "Epoch  32 / iter   3, loss = 0.1496\n",
      "Epoch  32 / iter   4, loss = 0.0846\n",
      "Epoch  33 / iter   0, loss = 0.1420\n",
      "Epoch  33 / iter   1, loss = 0.1369\n",
      "Epoch  33 / iter   2, loss = 0.0962\n",
      "Epoch  33 / iter   3, loss = 0.1480\n",
      "Epoch  33 / iter   4, loss = 0.0687\n",
      "Epoch  34 / iter   0, loss = 0.1234\n",
      "Epoch  34 / iter   1, loss = 0.1028\n",
      "Epoch  34 / iter   2, loss = 0.1407\n",
      "Epoch  34 / iter   3, loss = 0.1528\n",
      "Epoch  34 / iter   4, loss = 0.0390\n",
      "Epoch  35 / iter   0, loss = 0.1113\n",
      "Epoch  35 / iter   1, loss = 0.1289\n",
      "Epoch  35 / iter   2, loss = 0.1733\n",
      "Epoch  35 / iter   3, loss = 0.0892\n",
      "Epoch  35 / iter   4, loss = 0.0456\n",
      "Epoch  36 / iter   0, loss = 0.1358\n",
      "Epoch  36 / iter   1, loss = 0.0782\n",
      "Epoch  36 / iter   2, loss = 0.1475\n",
      "Epoch  36 / iter   3, loss = 0.1294\n",
      "Epoch  36 / iter   4, loss = 0.0442\n",
      "Epoch  37 / iter   0, loss = 0.1136\n",
      "Epoch  37 / iter   1, loss = 0.0954\n",
      "Epoch  37 / iter   2, loss = 0.1542\n",
      "Epoch  37 / iter   3, loss = 0.1262\n",
      "Epoch  37 / iter   4, loss = 0.0452\n",
      "Epoch  38 / iter   0, loss = 0.1277\n",
      "Epoch  38 / iter   1, loss = 0.1361\n",
      "Epoch  38 / iter   2, loss = 0.1103\n",
      "Epoch  38 / iter   3, loss = 0.0920\n",
      "Epoch  38 / iter   4, loss = 0.4119\n",
      "Epoch  39 / iter   0, loss = 0.1054\n",
      "Epoch  39 / iter   1, loss = 0.1165\n",
      "Epoch  39 / iter   2, loss = 0.1334\n",
      "Epoch  39 / iter   3, loss = 0.1240\n",
      "Epoch  39 / iter   4, loss = 0.0672\n",
      "Epoch  40 / iter   0, loss = 0.1218\n",
      "Epoch  40 / iter   1, loss = 0.0982\n",
      "Epoch  40 / iter   2, loss = 0.1077\n",
      "Epoch  40 / iter   3, loss = 0.1062\n",
      "Epoch  40 / iter   4, loss = 0.4781\n",
      "Epoch  41 / iter   0, loss = 0.1541\n",
      "Epoch  41 / iter   1, loss = 0.1049\n",
      "Epoch  41 / iter   2, loss = 0.0979\n",
      "Epoch  41 / iter   3, loss = 0.1042\n",
      "Epoch  41 / iter   4, loss = 0.0397\n",
      "Epoch  42 / iter   0, loss = 0.0996\n",
      "Epoch  42 / iter   1, loss = 0.1031\n",
      "Epoch  42 / iter   2, loss = 0.1294\n",
      "Epoch  42 / iter   3, loss = 0.0980\n",
      "Epoch  42 / iter   4, loss = 0.1135\n",
      "Epoch  43 / iter   0, loss = 0.1521\n",
      "Epoch  43 / iter   1, loss = 0.1088\n",
      "Epoch  43 / iter   2, loss = 0.1089\n",
      "Epoch  43 / iter   3, loss = 0.0775\n",
      "Epoch  43 / iter   4, loss = 0.1444\n",
      "Epoch  44 / iter   0, loss = 0.0827\n",
      "Epoch  44 / iter   1, loss = 0.0875\n",
      "Epoch  44 / iter   2, loss = 0.1428\n",
      "Epoch  44 / iter   3, loss = 0.1002\n",
      "Epoch  44 / iter   4, loss = 0.0352\n",
      "Epoch  45 / iter   0, loss = 0.0917\n",
      "Epoch  45 / iter   1, loss = 0.1193\n",
      "Epoch  45 / iter   2, loss = 0.0933\n",
      "Epoch  45 / iter   3, loss = 0.1044\n",
      "Epoch  45 / iter   4, loss = 0.0064\n",
      "Epoch  46 / iter   0, loss = 0.1020\n",
      "Epoch  46 / iter   1, loss = 0.0913\n",
      "Epoch  46 / iter   2, loss = 0.0882\n",
      "Epoch  46 / iter   3, loss = 0.1170\n",
      "Epoch  46 / iter   4, loss = 0.0330\n",
      "Epoch  47 / iter   0, loss = 0.0696\n",
      "Epoch  47 / iter   1, loss = 0.0996\n",
      "Epoch  47 / iter   2, loss = 0.0948\n",
      "Epoch  47 / iter   3, loss = 0.1109\n",
      "Epoch  47 / iter   4, loss = 0.5095\n",
      "Epoch  48 / iter   0, loss = 0.0929\n",
      "Epoch  48 / iter   1, loss = 0.1220\n",
      "Epoch  48 / iter   2, loss = 0.1150\n",
      "Epoch  48 / iter   3, loss = 0.0917\n",
      "Epoch  48 / iter   4, loss = 0.0968\n",
      "Epoch  49 / iter   0, loss = 0.0732\n",
      "Epoch  49 / iter   1, loss = 0.0808\n",
      "Epoch  49 / iter   2, loss = 0.0896\n",
      "Epoch  49 / iter   3, loss = 0.1306\n",
      "Epoch  49 / iter   4, loss = 0.1896\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 获取数据\n",
    "train_data, test_data = load_data(\"./data/housing.data\")\n",
    "\n",
    "# 创建网络\n",
    "net = Network(13)\n",
    "# 启动训练\n",
    "losses = net.train(train_data, num_epochs=50, batch_size=100, eta=0.1)\n",
    "\n",
    "# 画出损失函数的变化趋势\n",
    "plot_x = np.arange(len(losses))\n",
    "plot_y = np.array(losses)\n",
    "plt.plot(plot_x, plot_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "本节我们详细介绍了如何使用Numpy实现梯度下降算法，构建并训练了一个简单的线性模型实现波士顿房价预测，可以总结出，使用神经网络建模房价预测有三个要点：\n",
    "\n",
    "* 构建网络，初始化参数$w$和$b$，定义预测和损失函数的计算方法。\n",
    "* 随机选择初始点，建立梯度的计算方法和参数更新方式。\n",
    "* 从总的数据集中抽取部分数据作为一个mini_batch，计算梯度并更新参数，不断迭代直到损失函数几乎不再下降。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('TF2': conda)",
   "language": "python",
   "name": "python36964bittf2condabb5e8804759547bbb0b8b22765ded733"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
